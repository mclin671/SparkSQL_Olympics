{"Event":"DBCEventLoggingListenerMetadata","Spark Version":"3.2.1","Timestamp":1664292018208,"Rollover Number":3,"SparkContext Id":4888391838058116525}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":97,"description":"show databases","details":"org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\nscala.collection.immutable.List.map(List.scala:293)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nShowNamespaces (1)\n\n\n(1) ShowNamespaces\nOutput [1]: [databaseName#7112]\nArguments: [databaseName#7112], com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@426f1b10\n\n","sparkPlanInfo":{"nodeName":"ShowNamespaces","simpleString":"ShowNamespaces [databaseName#7112], com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@426f1b10","children":[],"metadata":{},"metrics":[]},"time":1664292018203,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":97,"time":1664292018325,"errorMessage":""}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":98,"description":"show databases","details":"org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3143)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:266)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:100)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:115)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nCollectLimit (3)\n+- CommandResult (1)\n      +- ShowNamespaces (2)\n\n\n(1) CommandResult\nOutput [1]: [databaseName#7112]\nArguments: [databaseName#7112]\n\n(2) ShowNamespaces\nOutput [1]: [databaseName#7112]\nArguments: [databaseName#7112], com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@426f1b10\n\n(3) CollectLimit\nInput [1]: [databaseName#7112]\nArguments: 1000001\n\n","sparkPlanInfo":{"nodeName":"CollectLimit","simpleString":"CollectLimit 1000001","children":[{"nodeName":"CommandResult","simpleString":"CommandResult [databaseName#7112]","children":[],"metadata":{},"metrics":[{"name":"number of output rows","accumulatorId":2791,"metricType":"sum"}]}],"metadata":{},"metrics":[{"name":"shuffle records written","accumulatorId":2789,"metricType":"sum"},{"name":"records read","accumulatorId":2787,"metricType":"sum"},{"name":"local bytes read","accumulatorId":2785,"metricType":"size"},{"name":"fetch wait time","accumulatorId":2786,"metricType":"timing"},{"name":"remote bytes read","accumulatorId":2783,"metricType":"size"},{"name":"local blocks read","accumulatorId":2782,"metricType":"sum"},{"name":"remote blocks read","accumulatorId":2781,"metricType":"sum"},{"name":"remote bytes read to disk","accumulatorId":2784,"metricType":"size"},{"name":"shuffle bytes written","accumulatorId":2788,"metricType":"size"}]},"time":1664292018367,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":98,"time":1664292018380,"errorMessage":""}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":99,"description":"show tables in `default`","details":"org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\nscala.collection.immutable.List.map(List.scala:293)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nShowTables (1)\n\n\n(1) ShowTables\nOutput [3]: [database#7123, tableName#7124, isTemporary#7125]\nArguments: [database#7123, tableName#7124, isTemporary#7125], com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@3d344602, [default]\n\n","sparkPlanInfo":{"nodeName":"ShowTables","simpleString":"ShowTables [database#7123, tableName#7124, isTemporary#7125], com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@3d344602, [default]","children":[],"metadata":{},"metrics":[]},"time":1664292018738,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":99,"time":1664292018848,"errorMessage":""}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":100,"description":"show tables in `default`","details":"org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3143)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:266)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:100)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:115)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nCollectLimit (3)\n+- CommandResult (1)\n      +- ShowTables (2)\n\n\n(1) CommandResult\nOutput [3]: [database#7123, tableName#7124, isTemporary#7125]\nArguments: [database#7123, tableName#7124, isTemporary#7125]\n\n(2) ShowTables\nOutput [3]: [database#7123, tableName#7124, isTemporary#7125]\nArguments: [database#7123, tableName#7124, isTemporary#7125], com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@3d344602, [default]\n\n(3) CollectLimit\nInput [3]: [database#7123, tableName#7124, isTemporary#7125]\nArguments: 1000001\n\n","sparkPlanInfo":{"nodeName":"CollectLimit","simpleString":"CollectLimit 1000001","children":[{"nodeName":"CommandResult","simpleString":"CommandResult [database#7123, tableName#7124, isTemporary#7125]","children":[],"metadata":{},"metrics":[{"name":"number of output rows","accumulatorId":2802,"metricType":"sum"}]}],"metadata":{},"metrics":[{"name":"shuffle records written","accumulatorId":2800,"metricType":"sum"},{"name":"records read","accumulatorId":2798,"metricType":"sum"},{"name":"local bytes read","accumulatorId":2796,"metricType":"size"},{"name":"fetch wait time","accumulatorId":2797,"metricType":"timing"},{"name":"remote bytes read","accumulatorId":2794,"metricType":"size"},{"name":"local blocks read","accumulatorId":2793,"metricType":"sum"},{"name":"remote blocks read","accumulatorId":2792,"metricType":"sum"},{"name":"remote bytes read to disk","accumulatorId":2795,"metricType":"size"},{"name":"shuffle bytes written","accumulatorId":2799,"metricType":"size"}]},"time":1664292018872,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":100,"time":1664292018876,"errorMessage":""}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":101,"description":"show databases","details":"org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\nscala.collection.immutable.List.map(List.scala:293)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nShowNamespaces (1)\n\n\n(1) ShowNamespaces\nOutput [1]: [databaseName#7148]\nArguments: [databaseName#7148], com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@4bf14773\n\n","sparkPlanInfo":{"nodeName":"ShowNamespaces","simpleString":"ShowNamespaces [databaseName#7148], com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@4bf14773","children":[],"metadata":{},"metrics":[]},"time":1664292425334,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":101,"time":1664292425342,"errorMessage":""}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":102,"description":"show databases","details":"org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3143)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:266)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:100)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:115)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nCollectLimit (3)\n+- CommandResult (1)\n      +- ShowNamespaces (2)\n\n\n(1) CommandResult\nOutput [1]: [databaseName#7148]\nArguments: [databaseName#7148]\n\n(2) ShowNamespaces\nOutput [1]: [databaseName#7148]\nArguments: [databaseName#7148], com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@4bf14773\n\n(3) CollectLimit\nInput [1]: [databaseName#7148]\nArguments: 1000001\n\n","sparkPlanInfo":{"nodeName":"CollectLimit","simpleString":"CollectLimit 1000001","children":[{"nodeName":"CommandResult","simpleString":"CommandResult [databaseName#7148]","children":[],"metadata":{},"metrics":[{"name":"number of output rows","accumulatorId":2813,"metricType":"sum"}]}],"metadata":{},"metrics":[{"name":"shuffle records written","accumulatorId":2811,"metricType":"sum"},{"name":"records read","accumulatorId":2809,"metricType":"sum"},{"name":"local bytes read","accumulatorId":2807,"metricType":"size"},{"name":"fetch wait time","accumulatorId":2808,"metricType":"timing"},{"name":"remote bytes read","accumulatorId":2805,"metricType":"size"},{"name":"local blocks read","accumulatorId":2804,"metricType":"sum"},{"name":"remote blocks read","accumulatorId":2803,"metricType":"sum"},{"name":"remote bytes read to disk","accumulatorId":2806,"metricType":"size"},{"name":"shuffle bytes written","accumulatorId":2810,"metricType":"size"}]},"time":1664292425351,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":102,"time":1664292425368,"errorMessage":""}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":103,"description":"show tables in `default`","details":"org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\nscala.collection.immutable.List.map(List.scala:293)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nShowTables (1)\n\n\n(1) ShowTables\nOutput [3]: [database#7159, tableName#7160, isTemporary#7161]\nArguments: [database#7159, tableName#7160, isTemporary#7161], com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@1aa27ece, [default]\n\n","sparkPlanInfo":{"nodeName":"ShowTables","simpleString":"ShowTables [database#7159, tableName#7160, isTemporary#7161], com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@1aa27ece, [default]","children":[],"metadata":{},"metrics":[]},"time":1664292425670,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":103,"time":1664292425682,"errorMessage":""}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":104,"description":"show tables in `default`","details":"org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3143)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:266)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:100)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:115)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nCollectLimit (3)\n+- CommandResult (1)\n      +- ShowTables (2)\n\n\n(1) CommandResult\nOutput [3]: [database#7159, tableName#7160, isTemporary#7161]\nArguments: [database#7159, tableName#7160, isTemporary#7161]\n\n(2) ShowTables\nOutput [3]: [database#7159, tableName#7160, isTemporary#7161]\nArguments: [database#7159, tableName#7160, isTemporary#7161], com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@1aa27ece, [default]\n\n(3) CollectLimit\nInput [3]: [database#7159, tableName#7160, isTemporary#7161]\nArguments: 1000001\n\n","sparkPlanInfo":{"nodeName":"CollectLimit","simpleString":"CollectLimit 1000001","children":[{"nodeName":"CommandResult","simpleString":"CommandResult [database#7159, tableName#7160, isTemporary#7161]","children":[],"metadata":{},"metrics":[{"name":"number of output rows","accumulatorId":2824,"metricType":"sum"}]}],"metadata":{},"metrics":[{"name":"shuffle records written","accumulatorId":2822,"metricType":"sum"},{"name":"records read","accumulatorId":2820,"metricType":"sum"},{"name":"local bytes read","accumulatorId":2818,"metricType":"size"},{"name":"fetch wait time","accumulatorId":2819,"metricType":"timing"},{"name":"remote bytes read","accumulatorId":2816,"metricType":"size"},{"name":"local blocks read","accumulatorId":2815,"metricType":"sum"},{"name":"remote blocks read","accumulatorId":2814,"metricType":"sum"},{"name":"remote bytes read to disk","accumulatorId":2817,"metricType":"size"},{"name":"shuffle bytes written","accumulatorId":2821,"metricType":"size"}]},"time":1664292425702,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":104,"time":1664292425712,"errorMessage":""}
