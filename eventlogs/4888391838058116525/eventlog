{"Event":"DBCEventLoggingListenerMetadata","Spark Version":"3.2.1","Timestamp":1664287709955,"Rollover Number":2,"SparkContext Id":4888391838058116525}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":85,"description":"describe `default`.`athlete_events`","details":"org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\nscala.collection.immutable.List.map(List.scala:293)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nExecute DescribeTableCommand (1)\n   +- DescribeTableCommand (2)\n\n\n(1) Execute DescribeTableCommand\nOutput [3]: [col_name#6956, data_type#6957, comment#6958]\n\n(2) DescribeTableCommand\nArguments: `spark_catalog`.`default`.`athlete_events`, false, [col_name#6956, data_type#6957, comment#6958]\n\n","sparkPlanInfo":{"nodeName":"Execute DescribeTableCommand","simpleString":"Execute DescribeTableCommand","children":[],"metadata":{},"metrics":[]},"time":1664287709942,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":85,"time":1664287709961,"errorMessage":""}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":86,"description":"describe `default`.`athlete_events`","details":"org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3143)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:266)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:100)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:115)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nCollectLimit (4)\n+- CommandResult (1)\n      +- Execute DescribeTableCommand (2)\n            +- DescribeTableCommand (3)\n\n\n(1) CommandResult\nOutput [3]: [col_name#6956, data_type#6957, comment#6958]\nArguments: [col_name#6956, data_type#6957, comment#6958]\n\n(2) Execute DescribeTableCommand\nOutput [3]: [col_name#6956, data_type#6957, comment#6958]\n\n(3) DescribeTableCommand\nArguments: `spark_catalog`.`default`.`athlete_events`, false, [col_name#6956, data_type#6957, comment#6958]\n\n(4) CollectLimit\nInput [3]: [col_name#6956, data_type#6957, comment#6958]\nArguments: 1001\n\n","sparkPlanInfo":{"nodeName":"CollectLimit","simpleString":"CollectLimit 1001","children":[{"nodeName":"CommandResult","simpleString":"CommandResult [col_name#6956, data_type#6957, comment#6958]","children":[],"metadata":{},"metrics":[{"name":"number of output rows","accumulatorId":2724,"metricType":"sum"}]}],"metadata":{},"metrics":[{"name":"shuffle records written","accumulatorId":2722,"metricType":"sum"},{"name":"records read","accumulatorId":2720,"metricType":"sum"},{"name":"local bytes read","accumulatorId":2718,"metricType":"size"},{"name":"fetch wait time","accumulatorId":2719,"metricType":"timing"},{"name":"remote bytes read","accumulatorId":2716,"metricType":"size"},{"name":"local blocks read","accumulatorId":2715,"metricType":"sum"},{"name":"remote blocks read","accumulatorId":2714,"metricType":"sum"},{"name":"remote bytes read to disk","accumulatorId":2717,"metricType":"size"},{"name":"shuffle bytes written","accumulatorId":2721,"metricType":"size"}]},"time":1664287710030,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":86,"time":1664287710037,"errorMessage":""}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":87,"description":"describe `default`.`noc_region`","details":"org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\nscala.collection.immutable.List.map(List.scala:293)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nExecute DescribeTableCommand (1)\n   +- DescribeTableCommand (2)\n\n\n(1) Execute DescribeTableCommand\nOutput [3]: [col_name#6986, data_type#6987, comment#6988]\n\n(2) DescribeTableCommand\nArguments: `spark_catalog`.`default`.`noc_region`, false, [col_name#6986, data_type#6987, comment#6988]\n\n","sparkPlanInfo":{"nodeName":"Execute DescribeTableCommand","simpleString":"Execute DescribeTableCommand","children":[],"metadata":{},"metrics":[]},"time":1664287710466,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":87,"time":1664287710484,"errorMessage":""}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":88,"description":"describe `default`.`noc_region`","details":"org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3143)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:266)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:100)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:115)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nCollectLimit (4)\n+- CommandResult (1)\n      +- Execute DescribeTableCommand (2)\n            +- DescribeTableCommand (3)\n\n\n(1) CommandResult\nOutput [3]: [col_name#6986, data_type#6987, comment#6988]\nArguments: [col_name#6986, data_type#6987, comment#6988]\n\n(2) Execute DescribeTableCommand\nOutput [3]: [col_name#6986, data_type#6987, comment#6988]\n\n(3) DescribeTableCommand\nArguments: `spark_catalog`.`default`.`noc_region`, false, [col_name#6986, data_type#6987, comment#6988]\n\n(4) CollectLimit\nInput [3]: [col_name#6986, data_type#6987, comment#6988]\nArguments: 1001\n\n","sparkPlanInfo":{"nodeName":"CollectLimit","simpleString":"CollectLimit 1001","children":[{"nodeName":"CommandResult","simpleString":"CommandResult [col_name#6986, data_type#6987, comment#6988]","children":[],"metadata":{},"metrics":[{"name":"number of output rows","accumulatorId":2735,"metricType":"sum"}]}],"metadata":{},"metrics":[{"name":"shuffle records written","accumulatorId":2733,"metricType":"sum"},{"name":"records read","accumulatorId":2731,"metricType":"sum"},{"name":"local bytes read","accumulatorId":2729,"metricType":"size"},{"name":"fetch wait time","accumulatorId":2730,"metricType":"timing"},{"name":"remote bytes read","accumulatorId":2727,"metricType":"size"},{"name":"local blocks read","accumulatorId":2726,"metricType":"sum"},{"name":"remote blocks read","accumulatorId":2725,"metricType":"sum"},{"name":"remote bytes read to disk","accumulatorId":2728,"metricType":"size"},{"name":"shuffle bytes written","accumulatorId":2732,"metricType":"size"}]},"time":1664287710519,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":88,"time":1664287710537,"errorMessage":""}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":89,"description":"describe `default`.`athlete_event_bronze`","details":"org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\nscala.collection.immutable.List.map(List.scala:293)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nDescribeTable (1)\n\n\n(1) DescribeTable\nArguments: [col_name#7004, data_type#7005, comment#7006], ResolvedTable com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@493d4749, default.athlete_event_bronze, DeltaTableV2(org.apache.spark.sql.SparkSession@1a7dcc1d,dbfs:/user/hive/warehouse/athlete_event_bronze,Some(CatalogTable(\nDatabase: default\nTable: athlete_event_bronze\nOwner: root\nCreated Time: Tue Sep 27 13:11:05 UTC 2022\nLast Access: UNKNOWN\nCreated By: Spark 3.2.1\nType: MANAGED\nProvider: delta\nTable Properties: [delta.lastCommitTimestamp=1664284260000, delta.lastUpdateVersion=0, delta.minReaderVersion=1, delta.minWriterVersion=2]\nStatistics: 6153552 bytes\nLocation: dbfs:/user/hive/warehouse/athlete_event_bronze\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog\nSchema: root\n-- ID: integer (nullable = true)\n-- Name: string (nullable = true)\n-- Sex: string (nullable = true)\n-- Age: string (nullable = true)\n-- Height: string (nullable = true)\n-- Weight: string (nullable = true)\n-- Team: string (nullable = true)\n-- NOC: string (nullable = true)\n-- Games: string (nullable = true)\n-- Year: string (nullable = true)\n-- Season: string (nullable = true)\n-- City: string (nullable = true)\n-- Sport: string (nullable = true)\n-- Event: string (nullable = true)\n-- Medal: string (nullable = true)\n)),Some(spark_catalog.default.athlete_event_bronze),None,Map(),org.apache.spark.sql.util.CaseInsensitiveStringMap@1f), [ID#7007, Name#7008, Sex#7009, Age#7010, Height#7011, Weight#7012, Team#7013, NOC#7014, Games#7015, Year#7016, Season#7017, City#7018, Sport#7019, Event#7020, Medal#7021], false\n\n","sparkPlanInfo":{"nodeName":"DescribeTable","simpleString":"DescribeTable [col_name#7004, data_type#7005, comment#7006], ResolvedTable com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@493d4749, default.athlete_event_bronze, DeltaTableV2(org.apache.spark.sql.SparkSession@1a7dcc1d,dbfs:/user/hive/warehouse/athlete_event_bronze,Some(CatalogTable(\nDatabase: default\nTable: athlete_event_bronze\nOwner: root\nCreated Time: Tue Sep 27 13:11:05 UTC 2022\nLast Access: UNKNOWN\nCreated By: Spark 3.2.1\nType: MANAGED\nProvider: delta\nTable Properties: [delta.lastCommitTimestamp=1664284260000, delta.lastUpdateVersion=0, delta.minReaderVersion=1, delta.minWriterVersion=2]\nStatistics: 6153552 bytes\nLocation: dbfs:/user/hive/warehouse/athlete_event_bronze\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog\nSchema: root\n |-- ID: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: string (nullable = true)\n |-- Height: string (nullable = true)\n |-- Weight: string (nullable = true)\n |-- Team: string (nullable = true)\n |-- NOC: string (nullable = true)\n |-- Games: string (nullable = true)\n |-- Year: string (nullable = true)\n |-- Season: string (nullable = true)\n |-- City: string (nullable = true)\n |-- Sport: string (nullable = true)\n |-- Event: string (nullable = true)\n |-- Medal: string (nullable = true)\n)),Some(spark_catalog.default.athlete_event_bronze),None,Map(),org.apache.spark.sql.util.CaseInsensitiveStringMap@1f), [ID#7007, Name#7008, Sex#7009, Age#7010, Height#7011, Weight#7012, Team#7013, NOC#7014, Games#7015, Year#7016, Season#7017, City#7018, Sport#7019, Event#7020, Medal#7021], false","children":[],"metadata":{},"metrics":[]},"time":1664287711039,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":89,"time":1664287711113,"errorMessage":""}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":90,"description":"describe `default`.`athlete_event_bronze`","details":"org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3143)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:266)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:100)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:115)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nCollectLimit (3)\n+- CommandResult (1)\n      +- DescribeTable (2)\n\n\n(1) CommandResult\nOutput [3]: [col_name#7004, data_type#7005, comment#7006]\nArguments: [col_name#7004, data_type#7005, comment#7006]\n\n(2) DescribeTable\nArguments: [col_name#7004, data_type#7005, comment#7006], ResolvedTable com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@493d4749, default.athlete_event_bronze, DeltaTableV2(org.apache.spark.sql.SparkSession@1a7dcc1d,dbfs:/user/hive/warehouse/athlete_event_bronze,Some(CatalogTable(\nDatabase: default\nTable: athlete_event_bronze\nOwner: root\nCreated Time: Tue Sep 27 13:11:05 UTC 2022\nLast Access: UNKNOWN\nCreated By: Spark 3.2.1\nType: MANAGED\nProvider: delta\nTable Properties: [delta.lastCommitTimestamp=1664284260000, delta.lastUpdateVersion=0, delta.minReaderVersion=1, delta.minWriterVersion=2]\nStatistics: 6153552 bytes\nLocation: dbfs:/user/hive/warehouse/athlete_event_bronze\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog\nSchema: root\n-- ID: integer (nullable = true)\n-- Name: string (nullable = true)\n-- Sex: string (nullable = true)\n-- Age: string (nullable = true)\n-- Height: string (nullable = true)\n-- Weight: string (nullable = true)\n-- Team: string (nullable = true)\n-- NOC: string (nullable = true)\n-- Games: string (nullable = true)\n-- Year: string (nullable = true)\n-- Season: string (nullable = true)\n-- City: string (nullable = true)\n-- Sport: string (nullable = true)\n-- Event: string (nullable = true)\n-- Medal: string (nullable = true)\n)),Some(spark_catalog.default.athlete_event_bronze),None,Map(),org.apache.spark.sql.util.CaseInsensitiveStringMap@1f), [ID#7007, Name#7008, Sex#7009, Age#7010, Height#7011, Weight#7012, Team#7013, NOC#7014, Games#7015, Year#7016, Season#7017, City#7018, Sport#7019, Event#7020, Medal#7021], false\n\n(3) CollectLimit\nInput [3]: [col_name#7004, data_type#7005, comment#7006]\nArguments: 1001\n\n","sparkPlanInfo":{"nodeName":"CollectLimit","simpleString":"CollectLimit 1001","children":[{"nodeName":"CommandResult","simpleString":"CommandResult [col_name#7004, data_type#7005, comment#7006]","children":[],"metadata":{},"metrics":[{"name":"number of output rows","accumulatorId":2746,"metricType":"sum"}]}],"metadata":{},"metrics":[{"name":"shuffle records written","accumulatorId":2744,"metricType":"sum"},{"name":"records read","accumulatorId":2742,"metricType":"sum"},{"name":"local bytes read","accumulatorId":2740,"metricType":"size"},{"name":"fetch wait time","accumulatorId":2741,"metricType":"timing"},{"name":"remote bytes read","accumulatorId":2738,"metricType":"size"},{"name":"local blocks read","accumulatorId":2737,"metricType":"sum"},{"name":"remote blocks read","accumulatorId":2736,"metricType":"sum"},{"name":"remote bytes read to disk","accumulatorId":2739,"metricType":"size"},{"name":"shuffle bytes written","accumulatorId":2743,"metricType":"size"}]},"time":1664287711138,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":90,"time":1664287711148,"errorMessage":""}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":91,"description":"describe `default`.`athlete_event_silver`","details":"org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\nscala.collection.immutable.List.map(List.scala:293)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nDescribeTable (1)\n\n\n(1) DescribeTable\nArguments: [col_name#7044, data_type#7045, comment#7046], ResolvedTable com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@1ded2ab5, default.athlete_event_silver, DeltaTableV2(org.apache.spark.sql.SparkSession@1a7dcc1d,dbfs:/user/hive/warehouse/athlete_event_silver,Some(CatalogTable(\nDatabase: default\nTable: athlete_event_silver\nOwner: root\nCreated Time: Tue Sep 27 13:11:29 UTC 2022\nLast Access: UNKNOWN\nCreated By: Spark 3.2.1\nType: MANAGED\nProvider: delta\nTable Properties: [delta.lastCommitTimestamp=1664284285000, delta.lastUpdateVersion=0, delta.minReaderVersion=1, delta.minWriterVersion=2]\nStatistics: 4294 bytes\nLocation: dbfs:/user/hive/warehouse/athlete_event_silver\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog\nSchema: root\n-- athlete_num: long (nullable = true)\n-- medal_num: long (nullable = true)\n-- NOC: string (nullable = true)\n)),Some(spark_catalog.default.athlete_event_silver),None,Map(),org.apache.spark.sql.util.CaseInsensitiveStringMap@1f), [athlete_num#7047L, medal_num#7048L, NOC#7049], false\n\n","sparkPlanInfo":{"nodeName":"DescribeTable","simpleString":"DescribeTable [col_name#7044, data_type#7045, comment#7046], ResolvedTable com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@1ded2ab5, default.athlete_event_silver, DeltaTableV2(org.apache.spark.sql.SparkSession@1a7dcc1d,dbfs:/user/hive/warehouse/athlete_event_silver,Some(CatalogTable(\nDatabase: default\nTable: athlete_event_silver\nOwner: root\nCreated Time: Tue Sep 27 13:11:29 UTC 2022\nLast Access: UNKNOWN\nCreated By: Spark 3.2.1\nType: MANAGED\nProvider: delta\nTable Properties: [delta.lastCommitTimestamp=1664284285000, delta.lastUpdateVersion=0, delta.minReaderVersion=1, delta.minWriterVersion=2]\nStatistics: 4294 bytes\nLocation: dbfs:/user/hive/warehouse/athlete_event_silver\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog\nSchema: root\n |-- athlete_num: long (nullable = true)\n |-- medal_num: long (nullable = true)\n |-- NOC: string (nullable = true)\n)),Some(spark_catalog.default.athlete_event_silver),None,Map(),org.apache.spark.sql.util.CaseInsensitiveStringMap@1f), [athlete_num#7047L, medal_num#7048L, NOC#7049], false","children":[],"metadata":{},"metrics":[]},"time":1664287711487,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":91,"time":1664287711516,"errorMessage":""}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":92,"description":"describe `default`.`athlete_event_silver`","details":"org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3143)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:266)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:100)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:115)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nCollectLimit (3)\n+- CommandResult (1)\n      +- DescribeTable (2)\n\n\n(1) CommandResult\nOutput [3]: [col_name#7044, data_type#7045, comment#7046]\nArguments: [col_name#7044, data_type#7045, comment#7046]\n\n(2) DescribeTable\nArguments: [col_name#7044, data_type#7045, comment#7046], ResolvedTable com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@1ded2ab5, default.athlete_event_silver, DeltaTableV2(org.apache.spark.sql.SparkSession@1a7dcc1d,dbfs:/user/hive/warehouse/athlete_event_silver,Some(CatalogTable(\nDatabase: default\nTable: athlete_event_silver\nOwner: root\nCreated Time: Tue Sep 27 13:11:29 UTC 2022\nLast Access: UNKNOWN\nCreated By: Spark 3.2.1\nType: MANAGED\nProvider: delta\nTable Properties: [delta.lastCommitTimestamp=1664284285000, delta.lastUpdateVersion=0, delta.minReaderVersion=1, delta.minWriterVersion=2]\nStatistics: 4294 bytes\nLocation: dbfs:/user/hive/warehouse/athlete_event_silver\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog\nSchema: root\n-- athlete_num: long (nullable = true)\n-- medal_num: long (nullable = true)\n-- NOC: string (nullable = true)\n)),Some(spark_catalog.default.athlete_event_silver),None,Map(),org.apache.spark.sql.util.CaseInsensitiveStringMap@1f), [athlete_num#7047L, medal_num#7048L, NOC#7049], false\n\n(3) CollectLimit\nInput [3]: [col_name#7044, data_type#7045, comment#7046]\nArguments: 1001\n\n","sparkPlanInfo":{"nodeName":"CollectLimit","simpleString":"CollectLimit 1001","children":[{"nodeName":"CommandResult","simpleString":"CommandResult [col_name#7044, data_type#7045, comment#7046]","children":[],"metadata":{},"metrics":[{"name":"number of output rows","accumulatorId":2757,"metricType":"sum"}]}],"metadata":{},"metrics":[{"name":"shuffle records written","accumulatorId":2755,"metricType":"sum"},{"name":"records read","accumulatorId":2753,"metricType":"sum"},{"name":"local bytes read","accumulatorId":2751,"metricType":"size"},{"name":"fetch wait time","accumulatorId":2752,"metricType":"timing"},{"name":"remote bytes read","accumulatorId":2749,"metricType":"size"},{"name":"local blocks read","accumulatorId":2748,"metricType":"sum"},{"name":"remote blocks read","accumulatorId":2747,"metricType":"sum"},{"name":"remote bytes read to disk","accumulatorId":2750,"metricType":"size"},{"name":"shuffle bytes written","accumulatorId":2754,"metricType":"size"}]},"time":1664287711537,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":92,"time":1664287711540,"errorMessage":""}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":93,"description":"show databases","details":"org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\nscala.collection.immutable.List.map(List.scala:293)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nShowNamespaces (1)\n\n\n(1) ShowNamespaces\nOutput [1]: [databaseName#7072]\nArguments: [databaseName#7072], com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@5a15ab64\n\n","sparkPlanInfo":{"nodeName":"ShowNamespaces","simpleString":"ShowNamespaces [databaseName#7072], com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@5a15ab64","children":[],"metadata":{},"metrics":[]},"time":1664290392128,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":93,"time":1664290392174,"errorMessage":""}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":94,"description":"show databases","details":"org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3143)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:266)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:100)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:115)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nCollectLimit (3)\n+- CommandResult (1)\n      +- ShowNamespaces (2)\n\n\n(1) CommandResult\nOutput [1]: [databaseName#7072]\nArguments: [databaseName#7072]\n\n(2) ShowNamespaces\nOutput [1]: [databaseName#7072]\nArguments: [databaseName#7072], com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@5a15ab64\n\n(3) CollectLimit\nInput [1]: [databaseName#7072]\nArguments: 1000001\n\n","sparkPlanInfo":{"nodeName":"CollectLimit","simpleString":"CollectLimit 1000001","children":[{"nodeName":"CommandResult","simpleString":"CommandResult [databaseName#7072]","children":[],"metadata":{},"metrics":[{"name":"number of output rows","accumulatorId":2768,"metricType":"sum"}]}],"metadata":{},"metrics":[{"name":"shuffle records written","accumulatorId":2766,"metricType":"sum"},{"name":"records read","accumulatorId":2764,"metricType":"sum"},{"name":"local bytes read","accumulatorId":2762,"metricType":"size"},{"name":"fetch wait time","accumulatorId":2763,"metricType":"timing"},{"name":"remote bytes read","accumulatorId":2760,"metricType":"size"},{"name":"local blocks read","accumulatorId":2759,"metricType":"sum"},{"name":"remote blocks read","accumulatorId":2758,"metricType":"sum"},{"name":"remote bytes read to disk","accumulatorId":2761,"metricType":"size"},{"name":"shuffle bytes written","accumulatorId":2765,"metricType":"size"}]},"time":1664290392207,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":94,"time":1664290392209,"errorMessage":""}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":95,"description":"show tables in `default`","details":"org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\nscala.collection.immutable.List.map(List.scala:293)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nShowTables (1)\n\n\n(1) ShowTables\nOutput [3]: [database#7083, tableName#7084, isTemporary#7085]\nArguments: [database#7083, tableName#7084, isTemporary#7085], com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@24b7f06f, [default]\n\n","sparkPlanInfo":{"nodeName":"ShowTables","simpleString":"ShowTables [database#7083, tableName#7084, isTemporary#7085], com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@24b7f06f, [default]","children":[],"metadata":{},"metrics":[]},"time":1664290392561,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":95,"time":1664290392627,"errorMessage":""}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart","executionId":96,"description":"show tables in `default`","details":"org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3143)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:266)\ncom.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:100)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:115)\ncom.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\ncom.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:631)\ncom.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\ncom.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\ncom.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\ncom.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\ncom.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:59)\ncom.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\ncom.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\ncom.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:59)\ncom.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:608)\ncom.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\nscala.util.Try$.apply(Try.scala:213)\ncom.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)","physicalPlanDescription":"== Physical Plan ==\nCollectLimit (3)\n+- CommandResult (1)\n      +- ShowTables (2)\n\n\n(1) CommandResult\nOutput [3]: [database#7083, tableName#7084, isTemporary#7085]\nArguments: [database#7083, tableName#7084, isTemporary#7085]\n\n(2) ShowTables\nOutput [3]: [database#7083, tableName#7084, isTemporary#7085]\nArguments: [database#7083, tableName#7084, isTemporary#7085], com.databricks.sql.managedcatalog.UnityCatalogV2Proxy@24b7f06f, [default]\n\n(3) CollectLimit\nInput [3]: [database#7083, tableName#7084, isTemporary#7085]\nArguments: 1000001\n\n","sparkPlanInfo":{"nodeName":"CollectLimit","simpleString":"CollectLimit 1000001","children":[{"nodeName":"CommandResult","simpleString":"CommandResult [database#7083, tableName#7084, isTemporary#7085]","children":[],"metadata":{},"metrics":[{"name":"number of output rows","accumulatorId":2779,"metricType":"sum"}]}],"metadata":{},"metrics":[{"name":"shuffle records written","accumulatorId":2777,"metricType":"sum"},{"name":"records read","accumulatorId":2775,"metricType":"sum"},{"name":"local bytes read","accumulatorId":2773,"metricType":"size"},{"name":"fetch wait time","accumulatorId":2774,"metricType":"timing"},{"name":"remote bytes read","accumulatorId":2771,"metricType":"size"},{"name":"local blocks read","accumulatorId":2770,"metricType":"sum"},{"name":"remote blocks read","accumulatorId":2769,"metricType":"sum"},{"name":"remote bytes read to disk","accumulatorId":2772,"metricType":"size"},{"name":"shuffle bytes written","accumulatorId":2776,"metricType":"size"}]},"time":1664290392660,"modifiedConfigs":{"spark.r.sql.derby.temp.dir":"/tmp/Rtmpim8F7B"}}
{"Event":"org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd","executionId":96,"time":1664290392673,"errorMessage":""}
