[driver] {
  // CAREFUL: the DriverDaemon will create a SparkContext using a combination this configuration and
  // spark.XXX options specified in
  // //spark/images/resources/databricks/spark/scripts/setup_driver.sh.
  // If the same configuration option is specified in both places, this file takes precedence.

  // Please note that DriverDaemon will only pick up confs starting with "spark".

  chauffeur.newMonitor = true

  "spark.databricks.acl.scim.client" = "com.databricks.spark.sql.acl.client.DriverToWebappScimClient"

  // PLAT-24701: We do not need Draining for Chauffeur
  "databricks.backend.common.drainJettyServersOnShutdown" = false

  // Quotes needed for non-hierarchical config options ("spark.speculation.quantile = 3" is
  // the same as "spark.speculation = { quantile = 3 }", which overrides "spark.speculation").
  "spark.speculation" = false
  "spark.speculation.quantile" = 0.9
  "spark.speculation.multiplier" = 3

  "spark.databricks.credential.redactor" = "com.databricks.logging.secrets.CredentialRedactorProxyImpl"
  "spark.databricks.redactor" = "com.databricks.spark.util.DatabricksSparkLogRedactorProxy"
  "spark.databricks.credential.aws.secretKey.redactor" = "com.databricks.spark.util.AWSSecretKeyRedactorProxy"

  // Enable directory commit by default
  "spark.sql.sources.commitProtocolClass" = "com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol"

  // Default token providers
  "spark.databricks.passthrough.s3a.tokenProviderClassName" = "com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider"
  "spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class" = "com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory"
  "spark.databricks.passthrough.adls.tokenProviderClassName" = "com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider"
  "spark.databricks.passthrough.adls.gen2.tokenProviderClassName" = "com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider"
  "spark.databricks.managedCatalog.s3a.tokenProviderClassName" = "com.databricks.backend.daemon.driver.credentials.ManagedCatalogS3TokenProvider"
  "spark.databricks.managedCatalog.adls.gen2.tokenProviderClassName" = "com.databricks.backend.daemon.driver.credentials.ManagedCatalogADLSTokenProvider"
  "spark.databricks.managedCatalog.gcs.tokenProviderClassName" = "com.databricks.backend.daemon.driver.credentials.ManagedCatalogGCSTokenProvider"
  "spark.databricks.passthrough.glue.credentialsProviderFactoryClassName" = "com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory"
  "spark.databricks.passthrough.glue.executorServiceFactoryClassName" = "com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory"

  // Credential passthrough token refresher
  "spark.databricks.passthrough.oauth.refresher.impl" = "com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient"

  // Enable preemption by default
  "spark.databricks.preemption.enabled" = true

  // Enable FS IO metrics by default
  "spark.databricks.metrics.filesystem_io_metrics" = true

  "spark.sql.allowMultipleContexts" = false
  "spark.driver.allowMultipleContexts" = false

  // These two configurations are defaults for PySpark. We explicitly set them here in order
  // to short-circuit past the DEFAULT_CONFIGS setting logic in PySpark's context.py:
  "spark.rdd.compress" = true
  "spark.serializer.objectStreamReset" = 100

  "spark.shuffle.manager" = "SORT"
  "spark.sql.parquet.cacheMetadata" = true
  "spark.sql.parquet.compression.codec" = "snappy"

  // [SC-7756]: When unregistering map outputs following a fetch failure, we should unregister
  // outputs from all executors on the same host, not just the executor which triggered the
  // fetch failure. In Databricks, shuffle files are served by the external shuffle service, which
  // may serve map outputs for multiple executors in case executors have died and been replaced.
  // For a discussion of the trade-offs involved in this decision, see the comments on SPARK-19753.
  "spark.files.fetchFailure.unRegisterOutputOnHost" = true

  // [ES-848] This was disabled by patching the source code in 1.6 db-spark
  "spark.shuffle.reduceLocality.enabled" = false

  // Enables the shuffle service in Spark, that runs as part of the Spark worker and provides a
  // server from which Executors can read shuffle files (rather than reading directly from
  // each other). This provides uninterrupted access to the files even when some executors are
  // turned off or killed. Note that since the external shuffle service runs inside the container,
  // force killing the container (e.g., while auto-scaling clusters) will not benefit from this.
  "spark.shuffle.service.enabled" = true
  "spark.shuffle.service.port" = 4048

  // This configuration matches an updated default in DBR, but we still must explicitly set it
  // here due to legacy configuration system reasons (see SC-7791)
  "spark.cleaner.referenceTracking.blocking" = false

  // Enables reading tables in Hive that are stored as Parquet files
  "spark.sql.hive.convertMetastoreParquet" = true

  // Use Spark SQL Native Data Sources for Hive CTAS statements
  "spark.sql.hive.convertCTAS" = true

  // Set Delta the default format
  "spark.sql.sources.default" = "delta"
  "spark.sql.legacy.createHiveTableByDefault" = false

  // Version of the Hive metastore.
  "spark.sql.hive.metastore.version" = "0.13.0"

  // PROD-5417: Set the shared prefixes for metastore class loader. This list contains various jdbc drivers
  // and DBC specific settings.
  // Please note that there are classes using microsoft.sql.DateTimeOffset or microsoft.sql.Types
  // prefix in the jdbc driver of sql server. So, we also put them in the shared prefixes.
  "spark.sql.hive.metastore.sharedPrefixes" = "org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks"

  // Configure Spark to use the right metastore JARs
  "spark.sql.hive.metastore.jars" = "/databricks/databricks-hive/*"

  // Increase the size of both the RPC timeout and block manager timeout
  "spark.rpc.message.maxSize" = 256
  "spark.storage.blockManagerTimeoutIntervalMs" = 300000

  "spark.driver.maxResultSize" = "4g"
  "spark.sql.streaming.stopTimeout" = "15s"

  // Per https://issues.apache.org/jira/browse/SPARK-8130
  // https://issues.apache.org/jira/browse/SPARK-8132
  "spark.files.useFetchCache" = false
  "spark.files.overwrite" = true

  // Per https://databricks.atlassian.net/browse/SC-1271. Fault tolerance in Spark Streaming
  "spark.streaming.driver.writeAheadLog.allowBatching" = true
  "spark.streaming.driver.writeAheadLog.closeFileAfterWrite" = true

  // LogStores for Delta
  "spark.databricks.tahoe.logStore.class" = "com.databricks.tahoe.store.DelegatingLogStore"
  "spark.databricks.tahoe.logStore.aws.class" = "com.databricks.tahoe.store.MultiClusterLogStore"
  "spark.databricks.tahoe.logStore.azure.class" = "com.databricks.tahoe.store.AzureLogStore"
  "spark.databricks.tahoe.logStore.gcp.class" = "com.databricks.tahoe.store.GCPLogStore"

  // Whether to throw a fatal error when trying to access S3 when on Azure with Delta. We throw an
  // error, because we need putIfAbsent guarantees on S3, and without the commit service, that's impossible.
  "spark.databricks.delta.logStore.crossCloud.fatal" = true
  // Enable multi cluster writes
  "spark.databricks.delta.multiClusterWrites.enabled" = true

  // Per https://databricks.atlassian.net/browse/PROD-6211
  "spark.r.numRBackendThreads" = 1

  "spark.sparklyr-backend.threads" = 1

  // Per https://databricks.atlassian.net/browse/ML-1927
  "spark.r.backendConnectionTimeout" = 604800 // 7 * 24 * 60 * 60 seconds

   // Per https://databricks.atlassian.net/browse/SC-3765
  "spark.sparkr.use.daemon" = false

  // Warehouse location. The value of this conf will also be propagated to
  // hive.metastore.warehouse.dir.
  "spark.sql.warehouse.dir" = "/user/hive/warehouse"

  // SC-4433: Increase Spark listener bus's event queue size to 20000
  "spark.scheduler.listenerbus.eventqueue.capacity" = 20000

  // Notebooks in the same cluster are in their own sessions
  "spark.databricks.session.share" = false

  // Log warnings and collect thread dumps when zombie tasks run for too long (SC-5313).
  "spark.task.reaper.enabled" = true
  "spark.task.reaper.killTimeout" = "60s"

  // [SC-6416] Disable logical overwrites for directory commit
  spark.databricks.io.directoryCommit.enableLogicalDelete = false

  // SQL ACL configs.
  "spark.databricks.acl.provider" = "com.databricks.sql.acl.ReflectionBackedAclProvider"
  "spark.databricks.acl.client" = "com.databricks.spark.sql.acl.client.SparkSqlAclClient"
  // SQL-based access control is disabled by default and is feature flagged via this flag.
  // IMPORTANT: For correctness, the following commit should be included in db-spark 2.1:
  // https://github.com/databricks/runtime/commit/7d0656aee72153e4963756b1d1c9466d4beea54c

  // NFS configuration for notebook python environments
  "spark.databricks.driverNfs.enabled" = true
  "spark.databricks.driverNfs.pathSuffix" = ".ephemeral_nfs"

  // Make Structured Streaming use DatabricksCheckpointFileManager for performing all checkpointing
  "spark.sql.streaming.checkpointFileManagerClass" = "com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager"

  "spark.databricks.service.dbutils.server.backend" = "com.databricks.dbconnect.SparkServerDBUtils"
  "spark.databricks.service.dbutils.repl.backend" = "com.databricks.dbconnect.ReplDBUtils"

  // Enable the use of %matplotlib inline magic command in python notebooks
  "spark.databricks.workspace.matplotlibInline.enabled" = true

  // Enable multiple results per cell to be populated
  "spark.databricks.workspace.multipleResults.enabled" = true

  // SC-15429: Enable Scala REPL class file cleanup
  "spark.databricks.repl.enableClassFileCleanup" = true

  // BI cloud fetch configuration
  "spark.databricks.cloudfetch.requesterClassName" = "com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester"

  // Properties written to Spark's hadoopConfiguration
  spark.hadoop {
    "mapred.output.committer.class" = "com.databricks.backend.daemon.data.client.DirectOutputCommitter"
    // PROD-5573:
    // Use MapReduceDirectOutputCommitter as the default output committer for FSBasedRelation. We set it
    // in Spark's hadoopConfiguration because output committer is set in the Hadoop configuration.
    // This has no impact to Spark 1.3.
    "spark.sql.sources.outputCommitterClass" = "com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter"
    // PROD-5573: Set the direct output committer for Parquet. This has no impact to Spark 1.3.
    // We need to set it in hadoop's configuration to make it work.
    "spark.sql.parquet.output.committer.class" = "org.apache.spark.sql.parquet.DirectParquetOutputCommitter"

    // Enable HTTP mode by default in 2.0 for the SQL proxy
    "hive.server2.transport.mode" = "http"
    "hive.server2.thrift.http.port" = 10000
    "hive.server2.use.SSL" = true
    "hive.server2.keystore.path" = "/databricks/keys/jetty_ssl_driver_keystore.jks"
    // SEC-573: This jks password is hardcoded and is not intended to protect the confidentiality
    // of the keystore. Do not assume the keystore file itself is protected.
    "hive.server2.keystore.password" = "gb1gQqZ9ZIHS"
    // SC-13350: don't use 'doAs' - our users don't correspond to system users
    "hive.server2.enable.doAs" = false

    // Expire the hive session after a period of non-activity. Note that due to ELB TCP idle
    // timeouts, we essentially have a <5 minute idle timeout anyways. The hive server code also
    // takes into account active queries, so this should never interrupt running jobs.
    "hive.server2.session.check.interval" = 60000
    "hive.server2.idle.session.timeout" = 900000  // 15 minutes

    // Interval after which a *completed* hive server operation is cleaned up. The hive default
    // is five days. This allows us to clean up resources sooner.
    "hive.server2.idle.operation.timeout" = 7200000 // 2 hours

    // Disable auth cookie generation - not used. (see SC-14846)
    "hive.server2.thrift.http.cookie.auth.enabled" = false

    // Hive Metastore connection retry default values
    "hive.hmshandler.retry.attempts" = 10
    "hive.hmshandler.retry.interval" = 2000

    // SC-4757: Per https://issues.apache.org/jira/browse/SPARK-10063, we need to set
    // mapreduce.fileoutputcommitter.algorithm.version to 2 to make FileOutputCommitter
    // have the same behavior as Hadoop 1.
    "mapreduce.fileoutputcommitter.algorithm.version" = 2

    // As S3 has no concept of file permissions, we should disable it to avoid unneeded warnings.
    // https://hortonworks.github.io/hdp-aws/s3-performance
    // It is false in Hive 0.13. But, we want to disable it explicitly in case we connect to
    // other versions of metastores.
    "hive.warehouse.subdir.inherit.perms" = false

    // SC-5316: Lowering the Parquet write buffer size. The default size is 0.95 (of the heap size),
    // which is too aggressive and causes instability.
    "parquet.memory.pool.ratio" = 0.5
    // SC-20924: Disable page size check estimate, it is enabled in parquet-mr fork by default.
    "parquet.page.size.check.estimate" = false
    // SC-20924: Check block size more frequently, default is 100 in parquet-mr fork.
    "parquet.block.size.row.check.min" = 10
    "parquet.block.size.row.check.max" = 10
    // SC-20133: Enable page level checksums on write and read path
    "parquet.page.write-checksum.enabled" = true
    "parquet.page.verify-checksum.enabled" = true
    "spark.databricks.io.parquet.verifyChecksumOnWrite.enabled" = false
    "spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException" = false
    "databricks.fs.perfMetrics.enable" = true

    // ES-234977: Parquet column index causes issues with DBR 10+ when using Databricks vectorised
    // reader, we still need to test the feature in DBR: https://databricks.atlassian.net/browse/SC-76283
    "parquet.filter.columnindex.enabled" = "false"

    // If a user connects via the sql proxy or DBConnect, the proxy servlet generates a user token
    // and places it in a header called X-Databricks-User-Token. When the Spark side receives the
    // thrift request, it copies that token from the header into a Spark context attribute called
    // spark.databricks.token
    // The header name must be the same as that used in SqlProxyServlet.SQL_PROXY_TOKEN_HEADER
    // This also copies the data about user storage credentials for Passthrough
    "spark.driverproxy.customHeadersToProperties" = "X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name"

    // Do not go through data daemon for S3A
    "fs.s3a.impl" = "shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem"
    "fs.cpfs-s3a.impl" = "com.databricks.sql.acl.fs.CredentialPassthroughFileSystem"
    "fs.mcfs-s3a.impl" = "com.databricks.sql.acl.fs.ManagedCatalogFileSystem"
    "fs.fcfs-s3a.impl" = "com.databricks.sql.acl.fs.FixedCredentialsFileSystem"
    "fs.s3n.impl" = "shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem"
    "fs.cpfs-s3n.impl" = "com.databricks.sql.acl.fs.CredentialPassthroughFileSystem"
    "fs.mcfs-s3n.impl" = "com.databricks.sql.acl.fs.ManagedCatalogFileSystem"
    "fs.fcfs-s3n.impl" = "com.databricks.sql.acl.fs.FixedCredentialsFileSystem"
    "fs.s3.impl" = "shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem"
    "fs.cpfs-s3.impl" = "com.databricks.sql.acl.fs.CredentialPassthroughFileSystem"
    "fs.mcfs-s3.impl" = "com.databricks.sql.acl.fs.ManagedCatalogFileSystem"
    "fs.fcfs-s3.impl" = "com.databricks.sql.acl.fs.FixedCredentialsFileSystem"
    # Disable the cache for the FixedCredentialsFileSystem, as it has to reinitialize a FileSystem
    # with specific credentials every time
    "fs.fcfs-s3a.impl.disable.cache" = true
    "fs.fcfs-s3.impl.disable.cache" = true
    "fs.fcfs-s3n.impl.disable.cache" = true

    "fs.elfs.impl" = "com.databricks.backend.daemon.data.client.unitycatalog.ExternalLocationFileSystem"
    "fs.elfs.impl.disable.cache" = true
    // Enable Faster S3 writer
    "fs.s3a.fast.upload" = true
    // Increase the maxinum number of connections to match with data daemon (200)
    "fs.s3a.connection.maximum" = 200
    // ES-6535: The maxinum number of threads sending S3 requests should not be greater than the
    // maxinum number of connections in the pool. Otherwise, a thread may not be able to get the
    // connection from the pool before timeout. The value here is picked to make sure the number of
    // uploading threads plus the number of reading threads (64, the max number of cores we support
    // in Databricks) is not greater than "fs.s3a.connection.maximum".
    "fs.s3a.threads.max" = 136
    // reduce the size of part and threshold to speed up the upload by using multipart-upload
    "fs.s3a.multipart.size" = 10485760
    "fs.s3a.multipart.threshold" = 104857600
    "fs.s3a.fast.upload.active.blocks" = 32
    "fs.s3a.attempts.maximum" = 10
    "fs.s3a.block.size" = 67108864
    "fs.s3a.connection.timeout" = 50000
    "fs.s3a.max.total.tasks" = 1000
    "fs.s3a.retry.limit" = 20
    "fs.s3a.retry.throttle.interval" = 500ms
    "fs.s3a.assumed.role.credentials.provider" = "com.amazonaws.auth.InstanceProfileCredentialsProvider"

    "fs.wasbs.impl" = "shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem"
    "fs.wasb.impl" = "shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem"
    "fs.abfs.impl" = "shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem"
    "fs.abfss.impl" = "shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem"
    "fs.cpfs-abfss.impl" = "com.databricks.sql.acl.fs.CredentialPassthroughFileSystem"
    "fs.mcfs-abfss.impl" = "com.databricks.sql.acl.fs.ManagedCatalogFileSystem"
    "fs.fcfs-wasb.impl" = "com.databricks.sql.acl.fs.FixedCredentialsFileSystem"
    "fs.fcfs-wasbs.impl" = "com.databricks.sql.acl.fs.FixedCredentialsFileSystem"
    "fs.fcfs-abfs.impl" = "com.databricks.sql.acl.fs.FixedCredentialsFileSystem"
    "fs.fcfs-abfss.impl" = "com.databricks.sql.acl.fs.FixedCredentialsFileSystem"
    "fs.adl.impl" = "com.databricks.adl.AdlFileSystem"
    "fs.cpfs-adl.impl" = "com.databricks.sql.acl.fs.CredentialPassthroughFileSystem"
    "fs.azure.cache.invalidator.type" = "com.databricks.encryption.utils.CacheInvalidatorImpl"
    "fs.azure.skip.metrics" = true
    "fs.azure.authorization.caching.enable" = false
    # Disable the cache for the FixedCredentialsFileSystem, as it has to reinitialize a FileSystem
    # with specific credentials every time
    "fs.fcfs-wasb.impl.disable.cache" = true
    "fs.fcfs-wasbs.impl.disable.cache" = true
    "fs.fcfs-abfs.impl.disable.cache" = true
    "fs.fcfs-abfss.impl.disable.cache" = true

    "fs.gs.impl" = "shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem"
    "fs.AbstractFileSystem.gs.impl" = "shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS"
    "fs.mcfs-gs.impl" = "com.databricks.sql.acl.fs.ManagedCatalogFileSystem"
    // SC-77294: Set upload chunk size to 16MB as it shows good performance with less risk of OOMs
    // on smaller worker nodes.
    "fs.gs.outputstream.upload.chunk.size" = 16777216

    "fs.file.impl" = "com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem"

    "fs.idbfs.impl" = "com.databricks.io.idbfs.IdbfsFileSystem"

    // SC-8093: Here we'd like to disable Hadoop internal `FileSystem` cache because:
    //
    //  1. Hadoop caches `FileSystem` instances using URIs as the cache keys, while
    //  2. Unlike AWS S3 URIs, Azure blob storage wasb/wasbs URIs propagate credentials using
    //     Hadoop `Configuration`s instead of encoding credentials as part of the URIs, therefore
    //  3. A `FileSystem` instance returned by a
    //
    //       Path#getFileSystem(Configuration)
    //
    //     call may return a wrong instance cached previously created using a `Configuration`
    //     containing unexpected credentials.
    "fs.wasb.impl.disable.cache" = true
    "fs.wasbs.impl.disable.cache" = true
    "fs.adl.impl.disable.cache" = true
    "fs.cpfs-adl.impl.disable.cache" = true
    "fs.abfs.impl.disable.cache" = true
    "fs.abfss.impl.disable.cache" = true
    "fs.cpfs-abfss.impl.disable.cache" = true
    "fs.mcfs-abfss.impl.disable.cache" = true
    "fs.gs.impl.disable.cache" = true

    "fs.elfs.impl" = "com.databricks.backend.daemon.data.client.unitycatalog.ExternalLocationFileSystem"
    "fs.elfs.impl.disable.cache" = true

    // SC-36485/SC-44407: Don't call `deleteUnnecessaryFakeDirectories` after creating a file.
    // Deleting fake directory markers slows down S3 file system operations, generates excessive
    // load on S3 partition, may trigger throttling because it creates tombstone markers, and may
    // slow down operations against versioned S3 buckets.
    "databricks.s3.create.deleteUnnecessaryFakeDirectories" = false

    // The following are legacy configurations that were part of core-site.xml before SC-81738
    "fs.dbfs.impl" = "com.databricks.backend.daemon.data.client.DBFS"
    "fs.dbfsartifacts.impl" = "com.databricks.backend.daemon.data.client.DBFSV1"

    // Disable ABFS improvements and corruption checks in parquet-mr
    "parquet.page.metadata.validation.enabled" = true
    "parquet.abfs.readahead.optimization.enabled" = false
  }

  // Configure metrics config path so that metrics are shipped to Ganglia
  "spark.metrics.conf" = "/databricks/spark/conf/metrics.properties"

  // Configure MC Client implementation
  "spark.databricks.managedCatalog.clientClassName" = "com.databricks.managedcatalog.ManagedCatalogClientImpl"

  // Configuration Nephos DBFS client class implementation
  "spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass" = "com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient"

  // Configuration Nephos IAM role client class implementation
  "spark.worker.aioaLazyConfig.iamReadinessCheckClientClass" = "com.databricks.backend.daemon.driver.NephosIamRoleCheckClient"

  // SQL configuration mapper class
  "spark.databricks.sql.configMapperClass" = "com.databricks.dbsql.config.SqlConfigMapperBridge"

  // Recipient profile provider class for managed deltasharing table
  "spark.delta.sharing.profile.provider.class" = "io.delta.sharing.DeltaSharingCredentialsProvider"
}
